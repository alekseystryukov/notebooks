{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"proxies.csv\", \"r\") as f:\n",
    "    proxies = f.read().split()\n",
    "    \n",
    "print(len(proxies))\n",
    "print(len(set(proxies)))\n",
    "\n",
    "with open(\"proxies.csv\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(set(proxies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.237.246.26:3128\n",
      "103.14.8.239:8080\n",
      "103.14.8.239:80\n",
      "115.124.73.122:80\n",
      "138.197.12.124:3128\n",
      "138.197.12.124:8080\n",
      "138.197.79.173:3128\n",
      "138.197.32.85:80\n",
      "138.197.79.173:8799\n",
      "138.197.32.85:8080\n",
      "138.197.79.173:80\n",
      "122.116.165.159:3128\n",
      "138.255.105.55:8080\n",
      "144.217.133.42:8080\n",
      "144.217.31.225:8080\n",
      "144.217.31.225:3128\n",
      "144.217.31.225:80\n",
      "138.201.63.123:31288\n",
      "144.217.96.81:8080\n",
      "144.217.96.81:3128\n",
      "149.56.81.59:8080\n",
      "152.194.72.134:8080\n",
      "124.244.39.44:3128\n",
      "151.252.120.177:8080\n",
      "158.69.78.208:3128\n",
      "158.69.78.208:8080\n",
      "158.69.78.208:80\n",
      "158.69.78.208:8799\n",
      "163.121.188.2:8080\n",
      "163.121.188.3:8080\n",
      "167.114.82.173:80\n",
      "167.114.82.173:3128\n",
      "176.159.230.207:3128\n",
      "168.187.239.10:3128\n",
      "144.76.106.53:80\n",
      "177.54.144.218:3128\n",
      "177.54.144.218:8799\n",
      "162.243.251.200:8080\n",
      "177.67.86.121:8080\n",
      "177.67.84.135:8080\n",
      "185.107.80.44:3128\n",
      "185.28.193.95:8080\n",
      "176.126.245.23:3128\n",
      "177.67.81.248:8799\n",
      "177.67.81.248:8080\n",
      "176.31.185.36:8080\n",
      "177.67.81.248:3128\n",
      "177.135.90.139:3128\n",
      "178.33.4.48:3128\n",
      "189.111.250.209:3128\n",
      "192.208.184.133:80\n",
      "192.169.144.40:8080\n",
      "192.81.213.244:3128\n",
      "192.95.20.192:80\n",
      "190.248.128.122:3128\n",
      "192.169.144.40:80\n",
      "194.9.26.237:8080\n",
      "12.129.82.194:8080\n",
      "199.48.160.69:3128\n",
      "201.149.108.226:8080\n",
      "207.154.205.235:3128\n",
      "207.99.118.74:8080\n",
      "201.48.34.195:3128\n",
      "200.29.191.149:3128\n",
      "202.188.101.4:3128\n",
      "211.54.3.133:3128\n",
      "212.47.248.110:8888\n",
      "218.144.101.42:3128\n",
      "192.241.176.181:8080\n",
      "202.140.132.5:80\n",
      "200.229.202.93:3128\n",
      "213.136.67.139:3128\n",
      "202.43.190.11:8118\n",
      "51.15.41.74:8080\n",
      "5.135.184.150:3128\n",
      "54.235.252.205:3128\n",
      "63.110.242.67:3128\n",
      "8.42.214.78:3128\n",
      "79.188.42.46:8080\n",
      "5.189.162.211:3128\n",
      "83.239.58.162:8080\n",
      "64.34.21.84:80\n",
      "45.40.143.57:80\n",
      "92.222.75.60:3128\n",
      "97.77.104.22:3128\n",
      "97.77.104.22:80\n",
      "86.121.255.215:3128\n",
      "64.150.191.81:3128\n",
      "89.163.246.150:8080\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "from lxml import html\n",
    "import csv\n",
    "\n",
    "with open(\"approved_proxies.csv\", \"r\") as f:\n",
    "    proxies = f.read().split()\n",
    "\n",
    "with open(\"proxies_https.csv\", \"w\") as f:    \n",
    "    for proxy in proxies:        \n",
    "        url = \"https://www.google.com/finance/getprices?i=60&p=10d&f=d,o,h,l,c,v&df=cpct&q=AAPL\"\n",
    "        try:\n",
    "            page = requests.get(url, timeout=6, proxies={'http': proxy, 'https': proxy})\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        else:    \n",
    "            lines = list(csv.reader(page.content.decode('utf-8').splitlines()))\n",
    "            if lines[3][0] != \"INTERVAL=60\":\n",
    "                print(page.content.decode('utf-8')[:100])\n",
    "            else:\n",
    "                f.write(\"{}\\n\".format(proxy))\n",
    "                print(proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import queue\n",
    "import threading\n",
    "    \n",
    "NUM_THREADS = 300\n",
    "\n",
    "def check_proxies():   \n",
    "    \n",
    "    tasks = queue.Queue() \n",
    "    responses = queue.Queue() \n",
    "    \n",
    "    with open(\"proxies.csv\", \"r\") as f:\n",
    "        proxies = f.read().split()\n",
    "    \n",
    "    for p in proxies:\n",
    "        tasks.put(p)\n",
    "    \n",
    "    def worker():\n",
    "        while True:\n",
    "            proxy = tasks.get()\n",
    "            if proxy is None:\n",
    "                break \n",
    "                \n",
    "            try:\n",
    "                base_url = \"http://www.nasdaq.com/symbol/{symbol}/time-sales?time={time}&pageno={pageno}\"\n",
    "                url = base_url.format(symbol=\"AAPL\", time=1, pageno=1)\n",
    "                page_res = requests.get(url, timeout=6, proxies={'http': \"http://{}\".format(proxy)})\n",
    "                tree = html.fromstring(page_res.content)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    select = tree.find('.//select[@id=\"quotes_content_left_DropDowntimerange\"]')\n",
    "                    if select is None or int(select.value) != 1:\n",
    "                        pass\n",
    "                    else:        \n",
    "                        tree = html.fromstring(page_res.content)\n",
    "                        table = tree.find('.//table[@id=\"AfterHoursPagingContents_Table\"]')\n",
    "                        if table is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            responses.put(proxy)\n",
    "                except Exception:\n",
    "                    print(e)\n",
    "                 \n",
    "            tasks.task_done()\n",
    "    \n",
    "\n",
    "    threads = []\n",
    "    for i in range(NUM_THREADS):\n",
    "        t = threading.Thread(target=worker)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # block until all tasks are done\n",
    "    tasks.join()\n",
    "\n",
    "    # stop workers\n",
    "    for i in range(NUM_THREADS):\n",
    "        tasks.put(None)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        \n",
    "    approved = []\n",
    "    while True:\n",
    "        try:\n",
    "            proxy = responses.get(block=False)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        else:\n",
    "            approved.append(proxy)\n",
    "            \n",
    "    print(len(approved))\n",
    "    with open(\"approved_proxies.csv\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(approved))\n",
    "\n",
    "check_proxies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
